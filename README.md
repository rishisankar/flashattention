# flashattention

Iterative implementations of [Flash Attention 2](https://arxiv.org/abs/2307.08691) in CUDA, optimizing for performance. Tested on a Nvidia A10G GPU on an Amazon EC2 g5.xlarge instance.

Testing single head attention (using M = 10000, N = 9000, d = 32). Performance comparisons to Pytorch:
- Naive Pytorch implementation (doing all operations of `softmax(Q * K.T / sqrt(d)) * V` in series): 150ms
- `torch.nn.functional.scaled_dot_product_attention`: 127ms
- Best implementation here: 6.95ms (>90% speedup!)

### Worklog (optimizing with Nsight Compute)

| Version | Optimization | Code | Duration | Compute Throughput % | Memory Throughput % | Notes |
| - | - | - | - | - | - | - |
| V1 | Baseline | [Link](./fa2_single_head_v1.cu) | 2.78s | 0.27% | 1.19% | Estimated speedup 98.75% since only 1 of 80 SMs being used. Compiling with `nvcc -o fa2_single_head_v1 fa2_single_head_v1.cu -lineinfo`.
| V2 | Parallelized work over multiple thread blocks | [Link](./fa2_single_head_v2.cu) | 35.74ms | 21.04% | 92.73% | Uncoalesced shared accesses est speedup 86.73%, shared load bank conflicts est speedup 78.20%, L1TEX local store access pattern est speedup 74.97%. Matrix multiplication is primary memory overhead.
| V3 | Matrix multiplication multiplies (A @ B) instead of (A @ B.T) | [Link](./fa2_single_head_v3.cu) | 9.48ms | 79.28% | 79.28% | L1TEX local store access pattern est speedup 55.43%; Memory I/O causing warp stalls. `matrix_block_load_transpose()` seems to have a big memory overhead.
| V4 | Faster matrix multiplication using registers based on https://siboehm.com/articles/22/CUDA-MMM | [Link](./fa2_single_head_v4.cu) | 43.98ms | 53.32% | 53.32% | Why is this slower than V3? Seems to be using local memory not registers.
| V5 | (builds off V3) Add padding to matrix load transpose to reduce smem bank store conflicts | [Link](./fa2_single_head_v5.cu) | 9.45ms | 79.61% | 79.61% | Matrix multiplication needs to be improved. li_update and mi_update also have excessive L1 wavefronts.
| V6 | V4 matmul code is correct but is using local memory (slow) because the indexing isn't computable at compile time ([ref](https://forums.developer.nvidia.com/t/nvcc-chooses-to-use-local-memory-while-there-is-a-lot-of-registers-it-can-use/198870)). | [Link](./fa2_single_head_v6.cu) | 6.95ms | 68.04% | 68.04% | Threads per block reduced to 512 to allow for more register space (only 64k per thread block, according to [technical specifications](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications)). However matmul is much faster so this is worth doing. New compile command: `nvcc -o fa2_single_head_v5 fa2_single_head_v5.cu -lineinfo -Xptxas -v -O3 -maxrregcount 128` to utilize as many registers as possible. Tried various blocktiling sizes (constant T), T=4 has best performance. Adding optimizations from V5 doesn't seem to help anymore - this builds off of V4.
| V7 | Use `cooperative_groups::memcpy_async` to load HBM to shared memory | [Link](./fa2_single_head_v7.cu) | 7.40ms | 63.93% | 63.93% | Async memory seems to be slower, unsure why.

### Testing

The python script `verify_output.py` computes the attention operation on the same input matrices in PyTorch (using the GPU), then verifies the output generated by the CUDA code matches it.

Steps for running the correctness test:
1. Compile the cuda program.
1. Run the cuda program with a filepath argument (ex: `./fa2_single_head_v1.cu result.out`). The output matrix O will be saved in the file.
2. Run the verify_output script with the filepath (ex: `python3 verify_output.py result.out`)

### Future
- Utilize tensor cores for matrix multiplication and switch to fp16
- Extend to multi-head attention
