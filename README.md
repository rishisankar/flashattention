# flashattention

Implementations of [Flash Attention 2](https://arxiv.org/abs/2307.08691) in CUDA. Tested on a Nvidia A10G GPU on an Amazon EC2 g5.xlarge instance.
